{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5b_3FlAkmtL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pexpect.replwrap import python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df=pd.read_csv('abcnews-date-text.csv',nrows=300000)\n",
    "text_data=df['headline_text'].astype(str).tolist()"
   ],
   "metadata": {
    "id": "BVXX0znrqOWi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1]\n",
    "    return \" \".join(words)\n"
   ],
   "metadata": {
    "id": "GCqb7MW4qZzW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cleaned_texts=[clean_text(t) for t in text_data]\n",
    "sentences=[t.split() for t in cleaned_texts if t.strip()!=\"\"]\n",
    "print(f\"exmple sentences: {sentences[0]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVoetzDyq05L",
    "outputId": "968179dd-cc48-4960-c4c2-93b3aa8afa8c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exmple sentences: ['aba', 'decides', 'community', 'broadcasting', 'licence']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "w2v_model= word2vec.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    sample=1e-4,\n",
    "    negative=10,\n",
    ")"
   ],
   "metadata": {
    "id": "bpqErtMrq1O0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(cleaned_texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "input_sequences=[]\n",
    "for line in cleaned_texts:\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "\n",
    "\n",
    "x=pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "x_train=x[:,:-1]\n",
    "y_train=x[:,-1]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(len(tokenizer.word_index))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGUZ4ti1rC2r",
    "outputId": "c34e1b28-e78f-4b26-82b7-f465b1cfa138"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1256468, 9)\n",
      "34448\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dim=w2v_model.vector_size\n",
    "embedding_matrix=np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "         embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "print(embedding_matrix.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eU4PvScyrDM0",
    "outputId": "2576216d-eaa3-4842-9500-909cedb62083"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(34449, 100)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_next_word=keras.models.Sequential()\n",
    "model_next_word.add(layers.Input(shape=(max_sequence_length- 1,)))\n",
    "model_next_word.add(layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "model_next_word.add(layers.LSTM(128))\n",
    "\n",
    "model_next_word.add(layers.Dense(vocab_size, activation='softmax'))"
   ],
   "metadata": {
    "id": "jvbhwkQirMuJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_next_word.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_next_word.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "Jrs4lnHGrNr6",
    "outputId": "e7f6b516-36b0-4c8a-9422-8df248e15ea0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001B[38;5;33mEmbedding\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m100\u001B[0m)         │     \u001B[38;5;34m3,444,900\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m117,248\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m34449\u001B[0m)          │     \u001B[38;5;34m4,443,921\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,444,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34449</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,443,921</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m8,006,069\u001B[0m (30.54 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,006,069</span> (30.54 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m8,006,069\u001B[0m (30.54 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,006,069</span> (30.54 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model_next_word.fit(x_train, y_train, epochs=30, verbose=1, batch_size=128,\n",
    "                validation_split=0.1,          callbacks=[early_stopping])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWOE3IbWrPlz",
    "outputId": "c941eab4-41b5-4e35-eeaf-3838a1b81372"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m140s\u001B[0m 16ms/step - accuracy: 0.0352 - loss: 7.7404 - val_accuracy: 0.0984 - val_loss: 6.6480\n",
      "Epoch 2/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m141s\u001B[0m 16ms/step - accuracy: 0.1072 - loss: 6.2628 - val_accuracy: 0.1124 - val_loss: 6.4494\n",
      "Epoch 3/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m135s\u001B[0m 15ms/step - accuracy: 0.1297 - loss: 5.8155 - val_accuracy: 0.1193 - val_loss: 6.4199\n",
      "Epoch 4/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m135s\u001B[0m 15ms/step - accuracy: 0.1461 - loss: 5.5226 - val_accuracy: 0.1188 - val_loss: 6.4583\n",
      "Epoch 5/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m142s\u001B[0m 15ms/step - accuracy: 0.1597 - loss: 5.3075 - val_accuracy: 0.1182 - val_loss: 6.5254\n",
      "Epoch 6/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m142s\u001B[0m 15ms/step - accuracy: 0.1739 - loss: 5.1275 - val_accuracy: 0.1182 - val_loss: 6.5984\n",
      "Epoch 7/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m138s\u001B[0m 16ms/step - accuracy: 0.1863 - loss: 4.9845 - val_accuracy: 0.1163 - val_loss: 6.6737\n",
      "Epoch 8/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m142s\u001B[0m 16ms/step - accuracy: 0.1991 - loss: 4.8582 - val_accuracy: 0.1150 - val_loss: 6.7515\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c7f401d93d0>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_next_words(model, tokenizer, max_sequence_len, seed_text, num_words_to_predict=1):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(num_words_to_predict):\n",
    "        cleaned_seed = clean_text(generated_text)\n",
    "        token_list = tokenizer.texts_to_sequences([cleaned_seed])[0]\n",
    "\n",
    "        if not token_list:\n",
    "            print(\"لم يتم العثور على كلمات صالحة في النص المدخل .\")\n",
    "            return generated_text\n",
    "\n",
    "\n",
    "        padded_token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted_probs = model.predict(padded_token_list, verbose=0)[0]\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "\n",
    "        if predicted_word:\n",
    "            generated_text += \" \" + predicted_word\n",
    "        else:\n",
    "            print(\"لم يتم العثور على كلمة متوقعة.\")\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n",
    "seed_1 = \"police investigate\"\n",
    "predicted_1 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_1, num_words_to_predict=3)\n",
    "print(f\"النص الأصلي: '{seed_1}' -> التوقع: '{predicted_1}'\")\n",
    "\n",
    "seed_2 = \"government announce\"\n",
    "predicted_2 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_2, num_words_to_predict=2)\n",
    "print(f\"النص الأصلي: '{seed_2}' -> التوقع: '{predicted_2}'\")\n",
    "\n",
    "seed_3 = \"australia\"\n",
    "predicted_3 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_3, num_words_to_predict=5)\n",
    "print(f\"النص الأصلي: '{seed_3}' -> التوقع: '{predicted_3}'\")\n",
    "\n",
    "seed_4 = \"global warming\"\n",
    "predicted_4 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_4, num_words_to_predict=4)\n",
    "print(f\"النص الأصلي: '{seed_4}' -> التوقع: '{predicted_4}'\")\n",
    "\n",
    "seed_5 = \"egypt\"\n",
    "predicted_5 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_5, num_words_to_predict=3)\n",
    "print(f\"النص الأصلي: '{seed_5}' -> التوقع: '{predicted_5}'\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSHdoVMerfgx",
    "outputId": "d25866e6-376d-4dfa-9fca-2300d27b0ed6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "النص الأصلي: 'police investigate' -> التوقع: 'police investigate fatal car crash'\n",
      "النص الأصلي: 'government announce' -> التوقع: 'government announce new law'\n",
      "النص الأصلي: 'australia' -> التوقع: 'australia take lead world cup final'\n",
      "النص الأصلي: 'global warming' -> التوقع: 'global warming may help ease water'\n",
      "النص الأصلي: 'egypt' -> التوقع: 'egypt train crash kill'\n"
     ]
    }
   ]
  }
 ]
}
