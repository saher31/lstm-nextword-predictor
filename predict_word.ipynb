{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N5b_3FlAkmtL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pexpect.replwrap import python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df=pd.read_csv('abcnews-date-text.csv',nrows=300000)\n",
    "text_data=df['headline_text'].astype(str).tolist()"
   ],
   "metadata": {
    "id": "BVXX0znrqOWi"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1]\n",
    "    return \" \".join(words)\n"
   ],
   "metadata": {
    "id": "GCqb7MW4qZzW"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cleaned_texts=[clean_text(t) for t in text_data]\n",
    "sentences=[t.split() for t in cleaned_texts if t.strip()!=\"\"]\n",
    "print(f\"exmple sentences: {sentences[0]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVoetzDyq05L",
    "outputId": "fb835bf5-2458-4db2-b92d-144aa7a4222a"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exmple sentences: ['aba', 'decides', 'community', 'broadcasting', 'licence']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "w2v_model= word2vec.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    sample=1e-4,\n",
    "    negative=10,\n",
    ")"
   ],
   "metadata": {
    "id": "bpqErtMrq1O0"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(cleaned_texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "input_sequences=[]\n",
    "for line in cleaned_texts:\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "\n",
    "\n",
    "x=pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "x_train=x[:,:-1]\n",
    "y_train=x[:,-1]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(len(tokenizer.word_index))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGUZ4ti1rC2r",
    "outputId": "b18ed176-60fa-47fe-9311-44940f6310be"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1256468, 9)\n",
      "34448\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dim=w2v_model.vector_size\n",
    "embedding_matrix=np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "         embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "print(embedding_matrix.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eU4PvScyrDM0",
    "outputId": "f1833641-041f-40fa-ac43-f8c757153f9b"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(34449, 100)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_next_word=keras.models.Sequential()\n",
    "model_next_word.add(layers.Input(shape=(max_sequence_length- 1,)))\n",
    "model_next_word.add(layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "model_next_word.add(layers.LSTM(128))\n",
    "\n",
    "model_next_word.add(layers.Dense(vocab_size, activation='softmax'))"
   ],
   "metadata": {
    "id": "jvbhwkQirMuJ"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_next_word.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_next_word.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "Jrs4lnHGrNr6",
    "outputId": "142168d4-dafd-4944-eaa0-528c07078b4c"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m100\u001B[0m)         │     \u001B[38;5;34m3,444,900\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m117,248\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m34449\u001B[0m)          │     \u001B[38;5;34m4,443,921\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,444,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34449</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,443,921</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m8,006,069\u001B[0m (30.54 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,006,069</span> (30.54 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m8,006,069\u001B[0m (30.54 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,006,069</span> (30.54 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model_next_word.fit(x_train, y_train, epochs=30, verbose=1, batch_size=128,\n",
    "                validation_split=0.1,          callbacks=[early_stopping])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWOE3IbWrPlz",
    "outputId": "b58b781e-c5a5-4771-ae84-32a912d32756"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m137s\u001B[0m 15ms/step - accuracy: 0.0356 - loss: 7.7438 - val_accuracy: 0.0979 - val_loss: 6.6545\n",
      "Epoch 2/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m143s\u001B[0m 16ms/step - accuracy: 0.1063 - loss: 6.2725 - val_accuracy: 0.1123 - val_loss: 6.4513\n",
      "Epoch 3/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m136s\u001B[0m 15ms/step - accuracy: 0.1295 - loss: 5.8205 - val_accuracy: 0.1179 - val_loss: 6.4345\n",
      "Epoch 4/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m142s\u001B[0m 15ms/step - accuracy: 0.1454 - loss: 5.5269 - val_accuracy: 0.1199 - val_loss: 6.4693\n",
      "Epoch 5/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m142s\u001B[0m 15ms/step - accuracy: 0.1595 - loss: 5.3081 - val_accuracy: 0.1192 - val_loss: 6.5309\n",
      "Epoch 6/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m142s\u001B[0m 15ms/step - accuracy: 0.1732 - loss: 5.1314 - val_accuracy: 0.1196 - val_loss: 6.6042\n",
      "Epoch 7/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m136s\u001B[0m 15ms/step - accuracy: 0.1863 - loss: 4.9847 - val_accuracy: 0.1179 - val_loss: 6.6867\n",
      "Epoch 8/30\n",
      "\u001B[1m8835/8835\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m143s\u001B[0m 16ms/step - accuracy: 0.1988 - loss: 4.8636 - val_accuracy: 0.1167 - val_loss: 6.7636\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7a0635758050>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_next_words(model, tokenizer, max_sequence_len, seed_text, num_words_to_predict=1):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(num_words_to_predict):\n",
    "        cleaned_seed = clean_text(generated_text)\n",
    "        token_list = tokenizer.texts_to_sequences([cleaned_seed])[0]\n",
    "\n",
    "        if not token_list:\n",
    "            print(\"No valid words found in the input text\")\n",
    "            return generated_text\n",
    "\n",
    "        padded_token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted_probs = model.predict(padded_token_list, verbose=0)[0]\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "\n",
    "        if predicted_word:\n",
    "            generated_text += \" \" + predicted_word\n",
    "        else:\n",
    "            print(\"No predicted word found\")\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "#Examples:\n",
    "seed_1 = \"police investigate\"\n",
    "predicted_1 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_1, num_words_to_predict=3)\n",
    "print(f\"Original: '{seed_1}' -> Prediction: '{predicted_1}'\")\n",
    "\n",
    "seed_2 = \"government announce\"\n",
    "predicted_2 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_2, num_words_to_predict=2)\n",
    "print(f\"Original: '{seed_2}' -> Prediction: '{predicted_2}'\")\n",
    "\n",
    "seed_3 = \"australia\"\n",
    "predicted_3 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_3, num_words_to_predict=5)\n",
    "print(f\"Original: '{seed_3}' -> Prediction: '{predicted_3}'\")\n",
    "\n",
    "seed_4 = \"global warming\"\n",
    "predicted_4 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_4, num_words_to_predict=4)\n",
    "print(f\"Original: '{seed_4}' -> Prediction: '{predicted_4}'\")\n",
    "\n",
    "seed_5 = \"egypt\"\n",
    "predicted_5 = predict_next_words(model_next_word, tokenizer, max_sequence_length, seed_5, num_words_to_predict=3)\n",
    "print(f\"Original: '{seed_5}' -> Prediction: '{predicted_5}'\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSHdoVMerfgx",
    "outputId": "c28dd4e2-91a7-4ee5-db25-ab0d705aa2d9"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original: 'police investigate' -> Prediction: 'police investigate fatal car crash'\n",
      "Original: 'government announce' -> Prediction: 'government announce new road'\n",
      "Original: 'australia' -> Prediction: 'australia set pace world cup final'\n",
      "Original: 'global warming' -> Prediction: 'global warming may help prevent alzheimers'\n",
      "Original: 'egypt' -> Prediction: 'egypt bus crash kill'\n"
     ]
    }
   ]
  }
 ]
}
